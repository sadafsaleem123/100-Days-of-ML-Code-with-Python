{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding Deep Learning Terminologies**\n",
        "\n",
        "Link: https://www.youtube.com/watch?v=7sB052Pz0sQ&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=1&ab_channel=AlexanderAmini"
      ],
      "metadata": {
        "id": "V0SvIPIbTv7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of adjusting the weights of a neural network based on input data and desired output, in order to improve the accuracy of the network's predictions is called the **training of the neural networks.**\n",
        "\n",
        "**Loss function:** This is a mathematical function that measures the error or difference between the predicted output of the neural network and the true output. The *empirical loss* measures the total loss over our entire dataset. *Cross-entropy loss* is commonly used with models that output a probability between 0 and 1, while *mean squared error loss* is used with regression models that output continuous real numbers. The goal is to minimize the loss function during training in order to improve the accuracy of the network's predictions.\n",
        "\n",
        "**Loss optimization:** This is the process of minimizing the error or loss incurred by the neural network when making predictions. This is done by adjusting the weights of the network.\n",
        "\n",
        "**Gradient descent:** This is an optimization algorithm used to find the minimum of a function, such as the loss function of a neural network. It involves iteratively adjusting the weights in the direction of the negative gradient of the loss function. The idea is to keep moving the weights in the direction that reduces the loss, until we reach the minimum.\n",
        "\n",
        "**Backpropagation: **This is a technique used to compute the gradients of the loss function with respect to the weights of the neural network, which is necessary for gradient-based optimization algorithms like gradient descent. It works by propagating the error backwards from the output layer to the input layer, using the chain rule of calculus to compute the gradients.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i7-gmLEPTwBe"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}