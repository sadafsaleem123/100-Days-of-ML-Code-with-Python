{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Deep Learning**\n",
        "\n",
        "*Artificial Intelligence > Machine Learning > Deep Learning*"
      ],
      "metadata": {
        "id": "FAVhKkKcN0w3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today I learned about **deep learning**, which involves the use of neural networks that process input data through interconnected layers of nodes, with each layer performing a specific type of computation. The network learns by adjusting the **weights** between nodes during **training** using the **backpropagation** algorithm and applying a non-linear activation function to the output of the linear combination of inputs and bias, with different **activation functions** having different characteristics and applications including sigmoid, hyperbolic tangent function and ReLU function."
      ],
      "metadata": {
        "id": "ZLbpco_uNbR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mathematical Equation: Forward Propogation**\n",
        "\n",
        "Forward Propagation is the process of passing input data through the neural network to produce an output.\n",
        "\n",
        "```\n",
        "y = g (wo + Sigma xi. wi)\n",
        "\n",
        "or, \n",
        "\n",
        "y = g (wo + Xt. W)*\n",
        "```\n",
        "**y: **is the output of the neural network, which is produced by applying the activation function g to the linear combination of inputs and bias.\n",
        "\n",
        "**xi:** refers to the i-th input feature or input variable.\n",
        "\n",
        "**wi:** is the weight associated with the i-th input feature or variable. The weight determines the strength of the connection between the input feature and the output of the neural network.\n",
        "\n",
        "**wo:** is the bias term, which is a constant value that is added to the linear combination of inputs. The bias helps the neural network to better model complex relationships in the data.\n",
        "\n",
        "**Sigma xi. wi:** This is the linear combination of the input features and their associated weights. This term is also sometimes referred to as the \"weighted sum\" of the inputs.\n",
        "\n",
        "**g:** is the non-linear activation function that is applied to the output of the linear combination. This is applied to the output of each node to introduce nonlinearity into the network.\n",
        "\n",
        "Some **activation function** includes Sigmoid Function, Hyperbolic Function, and Rectifies Linear Unit (ReLU)."
      ],
      "metadata": {
        "id": "rL_SzVm0u3nO"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}